week 1:


Week 2
Estimate population proportion with confidence
  Best estimate +/- Margin of Error
  Margin of Error = a few * estimated Standard Error
  a few depends on the signifiance level, e.g.Z* use 1.645 for 90% CI, 1.96 for 95% CI, 2.236 for 98% CI, 2.576 for 99% CI
Understanding confidence intervals
  We use our sample proportion to construct this interval but we do not have the full population, and not to refer to this interval as being as confidence interval for a sample proportion. It's a range of values to help us estimate what we think that population rate might be with a high level of confidence. 
  We still don't know the full population's true value. That's why we design experiment and gather representative data to estimate its value. 
  
Visual Theory

Brown university build visual tool to help see the theory
https://seeing-theory.brown.edu/
https://seeing-theory.brown.edu/frequentist-inference/index.html#section2

chapters -> Frequentist Inference ->  Interval Estimation



Professor reply:
################################################################################################################################################################################################
We use the z-test with proportions and the t-test with means in this course.

Some statisticians will use the z-test with means under certain conditions but this course does not cover those instances.

The below video from jbstatistics.com is a good discussion of why we should not use the z-test with means.  (All of the content on jbstatistics.com is excellent and highly recommended.)

https://www.jbstatistics.com/hypothesis-tests-on-one-mean-t-or-z/
#################################################################################################################################################################################################


Week 2 Reading 1
Confidence Intervals: Other Considerations
Confidence intervals and coverage probabilities
A confidence interval (CI) is an interval of the form (a, b), that is constructed from the data.  The purpose of a CI is to cover an unknown population parameter with “high probability” (we use probability here since it is not possible to construct an interval that is guaranteed to always cover the population parameter of interest). For example, if we want to estimate the mean body mass index (BMI) in a population of people, where the true mean BMI is 25.5, then the interval (24, 26.2) would cover the target, while the interval (26.1, 28) would not.

The confidence interval is defined through its lower confidence bound (LCB) and its upper confidence bound (UCB), which are both functions of the data. The population parameter of interest, denoted here by θ (theta), is an unknown constant. In the example above, θ = 25.5.  The “coverage probability” of the confidence interval is the probability that LCB <= θ <= UCB, written P(LCB <= θ <= UCB). The coverage probability is set by the researcher, and in most cases will be set to 95%.

The coverage probability is defined in terms of (hypothetical) repeated sampling of multiple data sets from the population of interest.  Over many such repeated samples, constructing one CI from each sample, there will be a fraction of the confidence intervals that cover the target.  This fraction is the coverage probability.

A wider confidence interval will have an easier time covering the target than a narrower one. On the other hand, a very wide interval is not very informative (imagine if we reported the fraction of voters supporting a particular candidate in an election as 55%, with a 95% CI spanning from 2% to 98%). Thus, the primary goal when constructing an interval is to “adapt to the data”, yielding a wider interval when the power is low and uncertainty is high, and a narrower interval when the power is high and uncertainty is low.

Ideally the “actual” coverage probability of a confidence interval obtained in practice will match the intended or “nominal” coverage probability. But a CI may fail to perform as desired. This is because a CI may be used in a setting where the conditions under which it was derived are violated. Here we will explore some common reasons why this may occur.  

The actual coverage probability of a confidence interval may be either less than the nominal coverage level (yielding an “anti-conservative” interval), or greater than the nominal coverage level (yielding a “conservative” interval). Although a conservative interval is often viewed slightly more favorably than an anti-conservative interval, both of these outcomes are undesirable -- we wish to obtain an interval whose actual coverage is as close as possible to the nominal coverage probability.

It is important to reiterate that in practice, we obtain one confidence interval from one sample.  This CI either covers or fails to cover the target value. For a specific data set, we do not know whether the CI derived from it actually covers the target value, but this is something that is either true or false - there is no probability involved when discussing whether one specific CI covers the target value.  

We rarely have multiple independent samples from the same population, so we cannot usually verify that a confidence interval attains its intended coverage probability. To reassure ourselves that the desired coverage is attained, we can study the theoretical properties that would be guaranteed to result in the intended coverage rate being achieved. We can also use computer simulations to assess how a given method for constructing CIs performs in various hypothetical settings. Statisticians make use of both of these approaches when assessing the performance of confidence intervals in particular settings.

The confidence intervals we have seen so far are all constructed using two key quantities: 

an unbiased estimate of a population parameter, and 

the standard error of this estimate.  

For example, if we are interested in estimating the population mean based on an independent and identically distributed (iid) sample of data, the unbiased estimate is the sample mean (x̄ or x_bar), and the standard error of this estimate is s/sqrt(n) (or σ/√n), where s is the standard deviation of the data, and n is the sample size.

Many confidence intervals are constructed using the form “point estimate +/- K standard errors."  For example, when working with the sample mean x̄ (x_bar), the interval is x̄ +/- 1.96 σ/√n.  The constant K is chosen to give the desired level of coverage. Specifically, we need the “Z-score” (x̄ - mu) / s to fall between -K and K with probability alpha. As long as this holds, then the interval x̄ +/- K *σ/√n will have the intended coverage probability.  The constant K plays a very important role in determining the properties of a CI, and will be discussed in more detail below.

Constructing confidence intervals
There are two ways we can obtain values of K to use in constructing the CI. One approach is based on making the very strong assumption that the data are independent and identically distributed, and follow a normal (Gaussian) distribution. If this is the case, then the Z-score follows a Student-t distribution with n-1 degrees of freedom. If we set K equal to the 1 - (1 - ɑ)/2 quantile of the Student-t distribution with n-1 degrees of freedom, then the resulting interval will have the intended coverage rate. Values of K constructed from the Student t-distribution will range from 2 to 2.5 for 95% coverage intervals if the sample size is greater than 5 (samples smaller than 5 observations are rare in practice). Thus, most CIs will be constructed by taking a “margin of error” around the point estimate that is between 2 and 2.5 times the standard error.

An alternative and much more broadly applicable basis for obtaining a value for K is to use the “central limit theorem” (CLT). The CLT states that the sample mean of independent and identically distributed values will be approximately normally distributed.  The CLT also implies that the Z-score will be approximately normally distributed. Importantly, the CLT provides these guarantees even when the individual data values have distributions that are not normal, as long as the sample size is “sufficiently large.” There are some additional technical conditions needed for the CLT to be applicable, but we will not discuss them here.

Unfortunately, there is no universal rule that defines how large the sample size should be to invoke the central limit theorem. In general, if the data distribution is close to being normal, then the Z-scores will be close to normally-distributed even when the sample size is quite small (e.g. around 10). If the individual data values are far from being normally distributed (e.g. they are strongly skewed or have heavy tails), then the CLT may not be relevant until the sample size is larger, say around 50.

As long as we can justify invoking the CLT, it is appropriate to use the 1 - (1 - ɑ)/2 quantile of the normal distribution to define K, which leads to setting K=1.96 in order to achieve an (approximate) 95% coverage probability. Thus, normality of the individual data values is not needed for a CI to have good coverage properties. It is good practice to inspect the distribution of a sample before proceeding to construct a confidence interval for its mean, for example, by looking at a histogram or quantile plot of the data. But it is not necessary that this show a nearly-normal distribution in order for the confidence interval to be meaningful, unless the sample size is very small and the data are strongly non-normal.

Another common practice is to use K as calculated from the Student-t distribution, even when the data are not taken to be normal. The rationale for doing this is that even though the Z-scores do not follow a Student-t distribution in this setting, the values of K obtained using the t-distribution will always be slightly larger than 1.96. Thus, the coverage will be slightly higher when using the t-distribution to calculate K compared to when using the normal distribution.  Using a slightly larger value of K helps compensate for several possible factors that could lead to the Z-scores being slightly heavier-tailed than predicted by a normal distribution. As the sample size grows, the values of K obtained from the normal and t-distributions will become very similar. The distinction between using these two approaches is therefore mainly relevant when the sample size is smaller than around 50.  

Alternative procedures for challenging situations
There are a few ways to reduce the risk that strong non-normality will lead to confidence intervals with poor performance. In order to provide some exposure to the types of procedures that statisticians use to conduct inference in challenging situations, we discuss two of these approaches next.

When working with the sample proportions, it is common to add two extra “successes” and two extra “failures” to the data before calculating the proportion. Thus, if we observe 5 successes and 7 failures, instead of estimating the success rate as 5 / (5 + 7), we estimate it as 7 / (7 + 9).  The standard error is also estimated using this adjustment. The resulting confidence interval generally has better coverage properties than the usual CI when the sample size is small.  This interval is often called the “Agresti-Coull” interval, after its inventors.

When working with strongly skewed data, another practical technique for improving the coverage properties of intervals is to transform the data with a skew-reducing transformation, e.g. a log transformation, then calculate the interval in the usual way (as described above) using the transformed data. The resulting interval can be transformed back to the original scale by applying the inverse transformation to the LCB and UCB. For example, if the transformation is the natural logarithm, the inverse transformation would be to exponentiate (anti-log) the LCB and UCB.

Conclusion
In summary, although normality of the data can play a role in determining the coverage properties of a confidence interval, it is generally not a major factor unless the sample size is quite small (much smaller than 50), or if the data are strongly non-normal.  In most cases, other factors besides Gaussianity of the individual data values are more likely to give rise to sub-optimal coverage. Two such factors that can cause major problems with CI coverage probabilities are clustering or other forms of dependence in the data, and overt or hidden pre-testing or multiplicity in the analysis. Clustering will be discussed extensively in Course 3.  We will discuss multiplicity in Week 3 of this course.

Reading 2
What Affects the Standard Error of an Estimate?
Thus far in the specialization, we have been emphasizing the importance of the standard error of a statistical estimate for making inference about the parameter being estimated. Recall from Week 4 of the first course of this specialization that this quantity provides us with an estimate of the standard deviation of the sampling distribution of estimates that would arise if we had drawn repeated samples of the same size and computed the same estimate for each random sample. In a simplified sense, the standard error gives us a sense of the uncertainty associated with our estimate. Estimates with smaller standard errors are thus consider more precise.

So what exactly impacts a standard error in terms of a study design? Below is a list of design features that would generally affect the standard error of an estimate. There is clearly a cost component associated with this list, as some of the design features would certainly require more financial resources.

1) The variance of the variables of interest that are used to compute the estimate.
In general, the more variability that is associated with a given variable being measured, the more imprecise estimates based on that variable will be. This makes careful and precise measurement of the variables of interest very important for any given study.

2) The size of the sample.
Larger samples will tend to produce sampling distributions with less variability (or, in other words, estimates with smaller standard errors). The more sample that can be measured, the better, but we also have to think carefully about the first point above. Just because we have a set of “big data” does not mean that we have a collection of precise measurements. Very unusual measures (outliers) could have strong influence on the variance of a given variable, and this requires careful descriptive assessment.

3) The amount of dependence in the observations collected, possibly due to cluster sampling.
In studies were clusters of units with similar characteristics are measured, the data collected will not be entirely independent with a given cluster (neighborhood, clinic, school, etc.). This is because units coming from the same cluster will generally have similar values on the variables of interest, and this could happen for a variety of reasons. This lack of independence in the observations collected reduces our effective sample size; we don’t have as much unique information as the size of our sample would suggest. We can account for this dependence within clusters by using specialized statistical procedures to estimate standard errors in a way that accounts for cluster sampling. The same problem arises in longitudinal studies, where we collect repeated measurements from the same individuals over time. While it may look like we have a large sample of observations, many of these observations will be strongly correlated with each other, and we need to account for this. In general, with these types of clustered data, standard errors will tend to be much larger, because the estimates computed across different studies will entirely depend on what clusters are under study. If the clusters tend to vary substantially in terms of the measures of interest, the variability of the sampling distribution will increase! Furthermore, the larger the sample size selected from each cluster (and thus the smaller the sample of clusters), the larger the standard errors will tend to be.

4) The stratification of the target sample.
If we select a stratified sample from a target population (see Week 4 of Course 1), we will tend to produce estimates with increased precision, because we are removing between-stratum variance from the variability of our estimates by design! Stratification of samples is always an important consideration, for this reason.

5) The use of sampling weights to compute our estimates.
While sampling weights are often necessary to compute unbiased population estimates, the use of weights in estimation can inflate the variance of our estimates. We can use specialized statistical procedures to make sure that our standard errors reflect the uncertainty in our estimates due to weighting. In general, the higher the variability in our weights, the more variable our estimates will be.

These five features are generally the main drivers of standard errors, but other design features may also ultimately affect standard errors (e.g., imputation of missing data). We will touch on these throughout the specialization. 

To make these points clear, the figure below simulates nine sampling distributions for a population mean based on combinations of sample size (n = 500, 1000, and 5000) and the size of the clusters (no clusters, clusters with 10 units sampled from each, and clusters with 50 units sampled from each) in a cluster sample design. The effects of these design decisions on the variability of the sampling distributions is clear: with larger sample sizes (going down the columns), the spread of the sampling distribution shrinks (lower standard errors!). With larger clusters, the spread of the sampling distribution increases (higher standard errors!).


Reading 3
t-distributions vs. z-distributions
In this course, you may notice that we use z-distributions when you might have expected to see t-distributions. Please consider the following:
Due to the central limit theorem (CLT), the sample mean (xbar) is approximately normally distributed regardless of the distribution of the data.

In statistical inference, we are often interested in the distribution of sqrt(n) 
⋅
⋅ xbar / S , the standardized version of xbar, where S is the estimated standard deviation.

In some situations, sqrt(n) 
⋅
⋅ xbar / S has a distribution that is close to a t-distribution.

Like the z-distribution (standard normal distribution), a t-distribution has a smooth shape.

Like a z-distribution, a t-distribution is symmetric and has a mean of zero.

The t-distribution is defined by the degrees of freedom. These are related to the sample size. 

For t-distributions with small degrees of freedom (small sample sizes), the tails are “thicker or heavier” as compared to the z-distribution.  


The figure below compares the standard normal distribution (the black line) to the t-distribution with various degrees of freedom ranging from 2 to 29. You can compare the tails and humps for each distribution—when the degrees of freedom are small, the t-distribution has thicker or heavier tails and a lower hump compared to a standard normal (z) distribution. As the degrees of freedom increase, the standard normal distribution and the t-distribution look very similar.

Graph showing multiple t-distributions of varying degrees of freedom compared to a z-distribution.
Big Idea:
When sample sizes are reasonably large, a standard normal (z) distribution is often used in statistical methods in place of a t-distribution.




--week 3
reading
Hypothesis Testing: Other Considerations
Statistical hypothesis testing reflects the scientific method, adapted to the setting of research involving data analysis.  In this framework, a researcher makes a precise statement about the population of interest, then aims to falsify the statement.  In statistical hypothesis testing, the statement in question is the null hypothesis.  If we reject the null hypothesis, we have falsified it (to some degree of confidence).  According to the scientific method, falsifying a hypothesis should require an overwhelming amount of evidence against it.  If the data we observe are ambiguous, or are only weakly contradictory to the null hypothesis, we do not reject the null hypothesis.

The framework of formal hypothesis testing defines two distinct types of errors.  A type I error (false positive) occurs when the null hypothesis is true but is incorrectly rejected.  A type II error occurs when the null hypothesis is not rejected when it actually is false.  Most traditional methods for statistical inference aim to strictly control the probability of a type I error, usually at 5%.  While we also wish to minimize the probability of a type II error, this is a secondary priority to controlling the type I error.

All the standard statistical testing procedures perform well at controlling type I error under ideal conditions, but most of them can break down and give misleading results in practice.  That is, if we claim to be conducting a test that has a 5% false positive rate, is this the actual false positive rate?  As we discussed earlier in the setting of confidence intervals, several complications that can arise in practice will result in a statistical procedure not performing as intended.  In order to reduce the risk of this happening, statisticians use statistical theory and computer simulations to assess the operating characteristics of testing procedures in various challenging settings.

Normality of the data
One of the commonly-stated “assumptions” that is often raised as a caveat when presenting statistical findings is the issue of the data being normally distributed.  While it is true that in some circumstances, strongly non-normal data can cause statistical tests to be misleading, in most settings, the data are not required to follow a normal distribution.  In addition, there are other issues unrelated to normality that are potentially more likely to produce misleading results.

Concerns about normality primarily center on the calibration of rejection regions for a test statistic, or equivalently, the manner in which p-values are computed.  As with confidence intervals, the Z-score plays the central role.  To be concrete, suppose we are conducting a test comparing two population means using independent samples (a “two sample t-test”).  The corresponding sample means are x1_bar and x2_bar, and the test is based on the difference between them, which is x1_bar - x2_bar.  This difference has a standard error, which we denote here by s (there are a few different ways to compute this standard error, s can refer to any of them here).  The Z-score is (x1_bar - x2_bar) / s.

Under the very strong assumption that the data are normally distributed, the Z-score follows a Student t-distribution, with degrees of freedom depending on the way that the standard error was constructed (in most cases the degrees of freedom will be between m-2 and m, where m is the combined sample size of the two samples being compared).  If the data are not normally distributed, we can appeal to the central limit theorem (CLT), which states that the Z-score will be approximately normally distributed as long as the sample size is not too small.

As discussed earlier in the setting of confidence intervals, there is no universal rule that states when the sample size is large enough to justify invoking the CLT.  Rules of thumb between 20 and 50 are often stated.  A smaller sample size is sufficient to invoke the CLT when the data approximately follow a normal distribution, and a larger sample size is needed if the data are strongly non-Gaussian.

While normality is a consideration in some settings, it is mainly relevant when the sample size is small and the data are strongly non-Gaussian.  Other issues can cause statistical tests to break down in a broader range of settings, so normality of the data should be seen as one of several factors that can impact the performance of a statistical test, and is often a relatively minor one at that.  

A useful approach in practice is to use the Student t-distribution to calculate rejection regions and p-values, even when the data are not expected to follow a normal distribution.  Doing so is slightly conservative, in that the rejection region based on the t-distribution will be slightly smaller than the rejection region based on the normal distribution, and p-values based on the t-distribution will be slightly larger than p-values based on the normal distribution.  As the sample size grows beyond around 50, there is little practical difference between using the t-distribution and the normal distribution when carrying out statistical hypothesis tests.

Clustering and data dependence
One additional issue that can adversely impact the performance of a statistical test is the presence of unknown (or unmodeled) correlations or clustering in the data.  For example, if the data values are observed in sequence (e.g. over time), with each value possibly being correlated with its neighbors, then we have “autocorrelation”, which is a form of dependence.  Alternatively, we may have some form of grouping or clustering in the data.  Methods for addressing these issues will be discussed in Course 3 of this specialization.

Causality
Another issue to be aware of when conducting statistical tests is that of confounding and causality.  This issue is especially relevant when interpreting the results of a statistical test.  Suppose, for example, that two groups of people differ significantly in terms of some trait, e.g. people with fewer dental cavities are seen to have statistically lower risk of heart disease compared to people with more cavities.  It is important to note that this effect could be due to a lurking factor, or to some form of selection bias.  For example, the people with fewer cavities may be less likely to be smokers.  Note that this is arguably not a problem with the statistical hypothesis test itself -- it may well be true that people with fewer cavities are less likely to have heart disease (in the sense of there being a real association between these two factors).  Rather, it is an issue of what substantive conclusions may be drawn, especially when people draw a causal conclusion where one is not warranted.

Multiplicity
A third pitfall that arises with statistical hypothesis testing, as well as with other forms of statistical inference such as confidence intervals, is that of “multiplicity”, which we will discuss next.  Note that a variety of terms have been used to describe this issue, including “data dredging”, “multiple testing”, and “p hacking” (in reference to “p values”).

Most statistical inference procedures, including both confidence intervals and hypothesis testing, are based on an idealized research design in which a single analysis with narrow scope is conducted using a data set. In practice, data analysis often involves data exploration coupled with formal inference.  It is now widely accepted that doing this heedlessly can lead to misleading results, and in particular often leads to statistical evidence for research findings being overstated.  This is a large topic; here we comment on a few aspects of it, starting with two examples of how multiplicity in data analysis can cause problems.

Suppose that a researcher habitually conducts a two-sample t-test comparing group means, then reports a confidence interval for the difference in population means only if the null hypothesis of the t-test is rejected.  On one hand, this researcher is exhibiting good judgment, as it is often recommended to report an effect size along with the results of any hypothesis test (the point estimate of the population mean difference is an effect size in this setting).  However, the confidence intervals selected in this way will have lower than the nominal coverage probability.  This is an example of a broader issue sometimes called “selective inference”.

Suppose that there are natural ways to divide the population into subgroups.  For example, imagine that a researcher is interested in whether people who sleep less than 7 hours per night on average during one month have greater gain of body weight in the following year.  The researcher’s initial plan may have been to consider the general population, but perhaps the investigator then decides to carry out analyses separately in women and in men, in older people and in younger people, in smokers and in non-smokers, etc.  Although it is possible that hypothesized effect may actually be much stronger in some of these subgroups than in others, repeatedly testing the same data on different subgroups will be likely to give rise to falsely positive evidence for an association.  For example, if the researcher conducts 3 independent tests on the same data (e.g. on different subgroups of the data), with each test having a 5% false positive probability, then the probability that at least one of the tests will yield a false positive is over 14%.

In recent years, researchers have identified more and more ways that multiple-testing can arise in practice, corrupting statistical findings.  Almost always, multiple testing leads to overstatements of the confidence in findings, or to erroneous findings being reported.  Fortunately, there are many approaches to remedying this issue.  The easiest of these to apply is the Bonferroni correction, which essentially involves multiplying all p-values by the number of tests that were performed.  For example, if we conduct 5 hypothesis tests, and one of them yields a p-value of 0.02, then we should adjust this p-value to 0.1 (= 0.02 * 5).  Thus, this test which would have been deemed to be “statistically significant” if conducted in isolation will be not be seen as such following the Bonferroni adjustment.

Power 
A final consideration we will discuss here is statistical power.  Power is often defined in a narrow sense as the probability of rejecting the null hypothesis when the null hypothesis is false.  Loosely speaking, this is the probability of not making a type II error.  More broadly, power can refer to any aspect of the study design or data analysis that would make it more likely for meaningful results to be attained.  There is a branch of statistics focusing on formal “power analysis” that aims to develop concrete and quantitative ways to assess the statistical power in a given setting.  We will not delve into these methods here, and instead will discuss at a higher level how low power intersects with and exacerbates some of the other complicating considerations discussed above.

The type I error rate is controlled by the researcher (say at 5%), but this only represents the risk of drawing a false conclusion from a single test.  In recent years, the notion of the “false discovery rate” (FDR) has been advanced to understand how often the conclusions drawn in a research process involving multiple formal inferential procedures are mistaken.  Focusing on hypothesis tests, we can consider a situation where, for example, five tests are to be conducted.  If two of the underlying null hypotheses are false, but the power to reject them is low (say it is only 20%), then around 25% of all the rejected null hypotheses were incorrectly rejected.  This shows how the FDR is different than the type I error rate, which remains controlled here at 5%.  


People sometimes incorrectly believe that controlling the type I error rate at 5% means that there is only a 5% chance that any reported finding is wrong.  As illustrated here, the probability that a reported finding is wrong can be much higher than the type I error rate.  This “error inflation” is primarily driven by two factors -- a researcher who pursues hypotheses that are unlikely to be correct, and a researcher who carries out studies with low statistical power will both have higher FDR in their work overall.  The latter issue is in principle addressable by encouraging researchers to pursue fewer, but higher quality studies (i.e. to pursue fewer studies with larger sample sizes rather than many studies with small sample sizes).  The former issue is harder to address, but it reflects the fact that in some fields, especially difficult areas of science such as genetics and neuroscience, there is a poor fundamental understanding of the systems under study, which may lead to people speculating and pursuing hypotheses with weak theoretical grounding. 
