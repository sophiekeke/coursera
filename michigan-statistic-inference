week 1:


Week 2
Estimate population proportion with confidence
  Best estimate +/- Margin of Error
  Margin of Error = a few * estimated Standard Error
  a few depends on the signifiance level, e.g.Z* use 1.645 for 90% CI, 1.96 for 95% CI, 2.236 for 98% CI, 2.576 for 99% CI
Understanding confidence intervals
  We use our sample proportion to construct this interval but we do not have the full population, and not to refer to this interval as being as confidence interval for a sample proportion. It's a range of values to help us estimate what we think that population rate might be with a high level of confidence. 
  We still don't know the full population's true value. That's why we design experiment and gather representative data to estimate its value. 
  
Visual Theory

Brown university build visual tool to help see the theory
https://seeing-theory.brown.edu/
https://seeing-theory.brown.edu/frequentist-inference/index.html#section2

chapters -> Frequentist Inference ->  Interval Estimation



Professor reply:
################################################################################################################################################################################################
We use the z-test with proportions and the t-test with means in this course.

Some statisticians will use the z-test with means under certain conditions but this course does not cover those instances.

The below video from jbstatistics.com is a good discussion of why we should not use the z-test with means.  (All of the content on jbstatistics.com is excellent and highly recommended.)

https://www.jbstatistics.com/hypothesis-tests-on-one-mean-t-or-z/
#################################################################################################################################################################################################


Week 2 Reading 1
Confidence Intervals: Other Considerations
Confidence intervals and coverage probabilities
A confidence interval (CI) is an interval of the form (a, b), that is constructed from the data.  The purpose of a CI is to cover an unknown population parameter with “high probability” (we use probability here since it is not possible to construct an interval that is guaranteed to always cover the population parameter of interest). For example, if we want to estimate the mean body mass index (BMI) in a population of people, where the true mean BMI is 25.5, then the interval (24, 26.2) would cover the target, while the interval (26.1, 28) would not.

The confidence interval is defined through its lower confidence bound (LCB) and its upper confidence bound (UCB), which are both functions of the data. The population parameter of interest, denoted here by θ (theta), is an unknown constant. In the example above, θ = 25.5.  The “coverage probability” of the confidence interval is the probability that LCB <= θ <= UCB, written P(LCB <= θ <= UCB). The coverage probability is set by the researcher, and in most cases will be set to 95%.

The coverage probability is defined in terms of (hypothetical) repeated sampling of multiple data sets from the population of interest.  Over many such repeated samples, constructing one CI from each sample, there will be a fraction of the confidence intervals that cover the target.  This fraction is the coverage probability.

A wider confidence interval will have an easier time covering the target than a narrower one. On the other hand, a very wide interval is not very informative (imagine if we reported the fraction of voters supporting a particular candidate in an election as 55%, with a 95% CI spanning from 2% to 98%). Thus, the primary goal when constructing an interval is to “adapt to the data”, yielding a wider interval when the power is low and uncertainty is high, and a narrower interval when the power is high and uncertainty is low.

Ideally the “actual” coverage probability of a confidence interval obtained in practice will match the intended or “nominal” coverage probability. But a CI may fail to perform as desired. This is because a CI may be used in a setting where the conditions under which it was derived are violated. Here we will explore some common reasons why this may occur.  

The actual coverage probability of a confidence interval may be either less than the nominal coverage level (yielding an “anti-conservative” interval), or greater than the nominal coverage level (yielding a “conservative” interval). Although a conservative interval is often viewed slightly more favorably than an anti-conservative interval, both of these outcomes are undesirable -- we wish to obtain an interval whose actual coverage is as close as possible to the nominal coverage probability.

It is important to reiterate that in practice, we obtain one confidence interval from one sample.  This CI either covers or fails to cover the target value. For a specific data set, we do not know whether the CI derived from it actually covers the target value, but this is something that is either true or false - there is no probability involved when discussing whether one specific CI covers the target value.  

We rarely have multiple independent samples from the same population, so we cannot usually verify that a confidence interval attains its intended coverage probability. To reassure ourselves that the desired coverage is attained, we can study the theoretical properties that would be guaranteed to result in the intended coverage rate being achieved. We can also use computer simulations to assess how a given method for constructing CIs performs in various hypothetical settings. Statisticians make use of both of these approaches when assessing the performance of confidence intervals in particular settings.

The confidence intervals we have seen so far are all constructed using two key quantities: 

an unbiased estimate of a population parameter, and 

the standard error of this estimate.  

For example, if we are interested in estimating the population mean based on an independent and identically distributed (iid) sample of data, the unbiased estimate is the sample mean (x̄ or x_bar), and the standard error of this estimate is s/sqrt(n) (or σ/√n), where s is the standard deviation of the data, and n is the sample size.

Many confidence intervals are constructed using the form “point estimate +/- K standard errors."  For example, when working with the sample mean x̄ (x_bar), the interval is x̄ +/- 1.96 σ/√n.  The constant K is chosen to give the desired level of coverage. Specifically, we need the “Z-score” (x̄ - mu) / s to fall between -K and K with probability alpha. As long as this holds, then the interval x̄ +/- K *σ/√n will have the intended coverage probability.  The constant K plays a very important role in determining the properties of a CI, and will be discussed in more detail below.

Constructing confidence intervals
There are two ways we can obtain values of K to use in constructing the CI. One approach is based on making the very strong assumption that the data are independent and identically distributed, and follow a normal (Gaussian) distribution. If this is the case, then the Z-score follows a Student-t distribution with n-1 degrees of freedom. If we set K equal to the 1 - (1 - ɑ)/2 quantile of the Student-t distribution with n-1 degrees of freedom, then the resulting interval will have the intended coverage rate. Values of K constructed from the Student t-distribution will range from 2 to 2.5 for 95% coverage intervals if the sample size is greater than 5 (samples smaller than 5 observations are rare in practice). Thus, most CIs will be constructed by taking a “margin of error” around the point estimate that is between 2 and 2.5 times the standard error.

An alternative and much more broadly applicable basis for obtaining a value for K is to use the “central limit theorem” (CLT). The CLT states that the sample mean of independent and identically distributed values will be approximately normally distributed.  The CLT also implies that the Z-score will be approximately normally distributed. Importantly, the CLT provides these guarantees even when the individual data values have distributions that are not normal, as long as the sample size is “sufficiently large.” There are some additional technical conditions needed for the CLT to be applicable, but we will not discuss them here.

Unfortunately, there is no universal rule that defines how large the sample size should be to invoke the central limit theorem. In general, if the data distribution is close to being normal, then the Z-scores will be close to normally-distributed even when the sample size is quite small (e.g. around 10). If the individual data values are far from being normally distributed (e.g. they are strongly skewed or have heavy tails), then the CLT may not be relevant until the sample size is larger, say around 50.

As long as we can justify invoking the CLT, it is appropriate to use the 1 - (1 - ɑ)/2 quantile of the normal distribution to define K, which leads to setting K=1.96 in order to achieve an (approximate) 95% coverage probability. Thus, normality of the individual data values is not needed for a CI to have good coverage properties. It is good practice to inspect the distribution of a sample before proceeding to construct a confidence interval for its mean, for example, by looking at a histogram or quantile plot of the data. But it is not necessary that this show a nearly-normal distribution in order for the confidence interval to be meaningful, unless the sample size is very small and the data are strongly non-normal.

Another common practice is to use K as calculated from the Student-t distribution, even when the data are not taken to be normal. The rationale for doing this is that even though the Z-scores do not follow a Student-t distribution in this setting, the values of K obtained using the t-distribution will always be slightly larger than 1.96. Thus, the coverage will be slightly higher when using the t-distribution to calculate K compared to when using the normal distribution.  Using a slightly larger value of K helps compensate for several possible factors that could lead to the Z-scores being slightly heavier-tailed than predicted by a normal distribution. As the sample size grows, the values of K obtained from the normal and t-distributions will become very similar. The distinction between using these two approaches is therefore mainly relevant when the sample size is smaller than around 50.  

Alternative procedures for challenging situations
There are a few ways to reduce the risk that strong non-normality will lead to confidence intervals with poor performance. In order to provide some exposure to the types of procedures that statisticians use to conduct inference in challenging situations, we discuss two of these approaches next.

When working with the sample proportions, it is common to add two extra “successes” and two extra “failures” to the data before calculating the proportion. Thus, if we observe 5 successes and 7 failures, instead of estimating the success rate as 5 / (5 + 7), we estimate it as 7 / (7 + 9).  The standard error is also estimated using this adjustment. The resulting confidence interval generally has better coverage properties than the usual CI when the sample size is small.  This interval is often called the “Agresti-Coull” interval, after its inventors.

When working with strongly skewed data, another practical technique for improving the coverage properties of intervals is to transform the data with a skew-reducing transformation, e.g. a log transformation, then calculate the interval in the usual way (as described above) using the transformed data. The resulting interval can be transformed back to the original scale by applying the inverse transformation to the LCB and UCB. For example, if the transformation is the natural logarithm, the inverse transformation would be to exponentiate (anti-log) the LCB and UCB.

Conclusion
In summary, although normality of the data can play a role in determining the coverage properties of a confidence interval, it is generally not a major factor unless the sample size is quite small (much smaller than 50), or if the data are strongly non-normal.  In most cases, other factors besides Gaussianity of the individual data values are more likely to give rise to sub-optimal coverage. Two such factors that can cause major problems with CI coverage probabilities are clustering or other forms of dependence in the data, and overt or hidden pre-testing or multiplicity in the analysis. Clustering will be discussed extensively in Course 3.  We will discuss multiplicity in Week 3 of this course.

Reading 2
What Affects the Standard Error of an Estimate?
Thus far in the specialization, we have been emphasizing the importance of the standard error of a statistical estimate for making inference about the parameter being estimated. Recall from Week 4 of the first course of this specialization that this quantity provides us with an estimate of the standard deviation of the sampling distribution of estimates that would arise if we had drawn repeated samples of the same size and computed the same estimate for each random sample. In a simplified sense, the standard error gives us a sense of the uncertainty associated with our estimate. Estimates with smaller standard errors are thus consider more precise.

So what exactly impacts a standard error in terms of a study design? Below is a list of design features that would generally affect the standard error of an estimate. There is clearly a cost component associated with this list, as some of the design features would certainly require more financial resources.

1) The variance of the variables of interest that are used to compute the estimate.
In general, the more variability that is associated with a given variable being measured, the more imprecise estimates based on that variable will be. This makes careful and precise measurement of the variables of interest very important for any given study.

2) The size of the sample.
Larger samples will tend to produce sampling distributions with less variability (or, in other words, estimates with smaller standard errors). The more sample that can be measured, the better, but we also have to think carefully about the first point above. Just because we have a set of “big data” does not mean that we have a collection of precise measurements. Very unusual measures (outliers) could have strong influence on the variance of a given variable, and this requires careful descriptive assessment.

3) The amount of dependence in the observations collected, possibly due to cluster sampling.
In studies were clusters of units with similar characteristics are measured, the data collected will not be entirely independent with a given cluster (neighborhood, clinic, school, etc.). This is because units coming from the same cluster will generally have similar values on the variables of interest, and this could happen for a variety of reasons. This lack of independence in the observations collected reduces our effective sample size; we don’t have as much unique information as the size of our sample would suggest. We can account for this dependence within clusters by using specialized statistical procedures to estimate standard errors in a way that accounts for cluster sampling. The same problem arises in longitudinal studies, where we collect repeated measurements from the same individuals over time. While it may look like we have a large sample of observations, many of these observations will be strongly correlated with each other, and we need to account for this. In general, with these types of clustered data, standard errors will tend to be much larger, because the estimates computed across different studies will entirely depend on what clusters are under study. If the clusters tend to vary substantially in terms of the measures of interest, the variability of the sampling distribution will increase! Furthermore, the larger the sample size selected from each cluster (and thus the smaller the sample of clusters), the larger the standard errors will tend to be.

4) The stratification of the target sample.
If we select a stratified sample from a target population (see Week 4 of Course 1), we will tend to produce estimates with increased precision, because we are removing between-stratum variance from the variability of our estimates by design! Stratification of samples is always an important consideration, for this reason.

5) The use of sampling weights to compute our estimates.
While sampling weights are often necessary to compute unbiased population estimates, the use of weights in estimation can inflate the variance of our estimates. We can use specialized statistical procedures to make sure that our standard errors reflect the uncertainty in our estimates due to weighting. In general, the higher the variability in our weights, the more variable our estimates will be.

These five features are generally the main drivers of standard errors, but other design features may also ultimately affect standard errors (e.g., imputation of missing data). We will touch on these throughout the specialization. 

To make these points clear, the figure below simulates nine sampling distributions for a population mean based on combinations of sample size (n = 500, 1000, and 5000) and the size of the clusters (no clusters, clusters with 10 units sampled from each, and clusters with 50 units sampled from each) in a cluster sample design. The effects of these design decisions on the variability of the sampling distributions is clear: with larger sample sizes (going down the columns), the spread of the sampling distribution shrinks (lower standard errors!). With larger clusters, the spread of the sampling distribution increases (higher standard errors!).


Reading 3
t-distributions vs. z-distributions
In this course, you may notice that we use z-distributions when you might have expected to see t-distributions. Please consider the following:
Due to the central limit theorem (CLT), the sample mean (xbar) is approximately normally distributed regardless of the distribution of the data.

In statistical inference, we are often interested in the distribution of sqrt(n) 
⋅
⋅ xbar / S , the standardized version of xbar, where S is the estimated standard deviation.

In some situations, sqrt(n) 
⋅
⋅ xbar / S has a distribution that is close to a t-distribution.

Like the z-distribution (standard normal distribution), a t-distribution has a smooth shape.

Like a z-distribution, a t-distribution is symmetric and has a mean of zero.

The t-distribution is defined by the degrees of freedom. These are related to the sample size. 

For t-distributions with small degrees of freedom (small sample sizes), the tails are “thicker or heavier” as compared to the z-distribution.  


The figure below compares the standard normal distribution (the black line) to the t-distribution with various degrees of freedom ranging from 2 to 29. You can compare the tails and humps for each distribution—when the degrees of freedom are small, the t-distribution has thicker or heavier tails and a lower hump compared to a standard normal (z) distribution. As the degrees of freedom increase, the standard normal distribution and the t-distribution look very similar.

Graph showing multiple t-distributions of varying degrees of freedom compared to a z-distribution.
Big Idea:
When sample sizes are reasonably large, a standard normal (z) distribution is often used in statistical methods in place of a t-distribution.
